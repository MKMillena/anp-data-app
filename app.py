import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import io
import re
import numpy as np

# Configura√ß√£o da P√°gina
PAGE_TITLE = "ANP Produ√ß√£o de Petr√≥leo e G√°s"
DATA_URL = "https://dados.gov.br/dados/conjuntos-dados/producao-de-petroleo-e-gas-natural-por-poco"

# --- HELPER FUNCTIONS ---

def get_available_years():
    """
    Varre o site da ANP e agrupa todos os links de CSV por ano.
    Se houver arquivos separados (Terra/Mar) para o mesmo ano, guarda ambos.
    Retorna: dict { '2025': ['url1', 'url2'], '2024': ['url1'] ... }
    """
    try:
        response = requests.get(DATA_URL)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        years_links = {}
        
        for link in soup.find_all('a', href=True):
            text = link.get_text().strip()
            href = link['href']
            
            # Pula links que n√£o sejam CSV ou ZIP (alguns meses v√™m zipados)
            if not ('.csv' in href.lower() or '.zip' in href.lower()):
                continue

            # Tenta encontrar um ano (4 d√≠gitos entre 2000 e 2099) no texto do link
            # Ex: "Produ√ß√£o por Po√ßo - 2024" -> encontra 2024
            match = re.search(r'(20\d{2})', text)
            
            if match:
                year = match.group(1)
                
                # Filtro de seguran√ßa para pegar apenas links relevantes de produ√ß√£o
                keywords = ['producao', 'produ√ß√£o', 'po√ßo', 'poco', 'mar', 'terra']
                if any(k in href.lower() or k in text.lower() for k in keywords):
                    if year not in years_links:
                        years_links[year] = []
                    
                    # Evita duplicatas
                    if href not in years_links[year]:
                        years_links[year].append(href)

        # Ordena os anos do mais recente para o mais antigo
        return dict(sorted(years_links.items(), key=lambda item: item[0], reverse=True))

    except Exception as e:
        st.error(f"Erro ao buscar dados do site: {e}")
        return {}

def process_dataframe(df):
    """
    Limpa, converte tipos e adiciona c√°lculos de engenharia.
    """
    # 1. Padroniza√ß√£o de Nomes de Colunas (Remove espa√ßos extras e colchetes)
    df.columns = df.columns.str.replace(r'[\[\]]', '', regex=True).str.strip()
    
    # 2. Convers√£o Num√©rica (PT-BR -> Float)
    cols_to_convert = [
        "Produ√ß√£o de √ìleo (m¬≥)", "Produ√ß√£o de G√°s Associado (Mm¬≥)", 
        "Produ√ß√£o de G√°s N√£o Associado (Mm¬≥)", "Produ√ß√£o de √Ågua (m¬≥)", 
        "Inje√ß√£o de G√°s (Mm¬≥)", "Inje√ß√£o de √Ågua para Recupera√ß√£o Secund√°ria (m¬≥)", 
        "Inje√ß√£o de √Ågua para Descarte (m¬≥)", "Inje√ß√£o de G√°s Carb√¥nico (Mm¬≥)", 
        "Inje√ß√£o de Nitrog√™nio (Mm¬≥)", "Inje√ß√£o de Vapor de √Ågua (t)"
    ]
    
    valid_cols = [c for c in cols_to_convert if c in df.columns]
    
    for col in valid_cols:
        if not pd.api.types.is_numeric_dtype(df[col]):
            try:
                df[col] = df[col].astype(str).str.replace('.', '', regex=False).str.replace(',', '.', regex=False)
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0)
            except Exception:
                pass
    
    # 3. Remo√ß√£o de Colunas Irrelevantes
    cols_to_drop = [
        "Bacia", "Instala√ß√£o", "Estado", "Ambiente", 
        "Produ√ß√£o de Condensado (m¬≥)", "Inje√ß√£o de Pol√≠meros (m¬≥)", 
        "Inje√ß√£o de Outros Fluidos (m¬≥)"
    ]
    df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors='ignore')

    # 4. C√°lculos de Engenharia
    if 'Ano' in df.columns and 'M√™s' in df.columns:
        # Cria data para ordena√ß√£o
        df['Data_Temp'] = pd.to_datetime(df['Ano'].astype(str) + '-' + df['M√™s'].astype(str) + '-01', errors='coerce')
        df = df.sort_values(by=['Po√ßo', 'Data_Temp'])
        
        # Tempo (dias) e Np (Acumulado)
        df['tempo'] = df.groupby('Po√ßo')['Data_Temp'].transform(lambda x: (x - x.min()).dt.days)
        df['Np'] = df.groupby('Po√ßo')['Produ√ß√£o de √ìleo (m¬≥)'].cumsum()
        df = df.drop(columns=['Data_Temp'])
    else:
        df['tempo'] = 0
        df['Np'] = 0

    # RGO e RAO
    gas_total_m3 = (df.get("Produ√ß√£o de G√°s Associado (Mm¬≥)", 0) + df.get("Produ√ß√£o de G√°s N√£o Associado (Mm¬≥)", 0)) * 1000
    
    with np.errstate(divide='ignore', invalid='ignore'):
        df['RGO'] = gas_total_m3 / df['Produ√ß√£o de √ìleo (m¬≥)']
        df['RAO'] = df['Produ√ß√£o de √Ågua (m¬≥)'] / df['Produ√ß√£o de √ìleo (m¬≥)']
    
    df['RGO'] = df['RGO'].replace([np.inf, -np.inf], 0).fillna(0)
    df['RAO'] = df['RAO'].replace([np.inf, -np.inf], 0).fillna(0)

    # lnq
    df['lnq'] = np.nan
    mask_oleo = df['Produ√ß√£o de √ìleo (m¬≥)'] > 0
    df.loc[mask_oleo, 'lnq'] = np.log(df.loc[mask_oleo, 'Produ√ß√£o de √ìleo (m¬≥)'])
    
    return df

@st.cache_data(show_spinner=True)
def load_data(urls):
    """
    Baixa um ou m√∫ltiplos CSVs (ex: Terra + Mar) e combina em um √∫nico DataFrame.
    """
    all_dfs = []
    
    # Garante que urls seja uma lista
    if isinstance(urls, str):
        urls = [urls]
        
    for url in urls:
        try:
            response = requests.get(url)
            response.raise_for_status()
            
            # Se for ZIP, precisaria de tratamento extra, mas aqui focamos no CSV padr√£o
            # O pandas l√™ ZIP automaticamente se for um √∫nico arquivo dentro, 
            # mas se a URL terminar em .csv, lemos direto.
            
            content = io.BytesIO(response.content)
            
            try:
                df_temp = pd.read_csv(content, sep=',', encoding='windows-1252', on_bad_lines='skip')
            except UnicodeDecodeError:
                content.seek(0)
                df_temp = pd.read_csv(content, sep=',', encoding='utf-8', on_bad_lines='skip')
            except Exception:
                # Tenta ponto e v√≠rgula se falhar
                content.seek(0)
                df_temp = pd.read_csv(content, sep=';', encoding='latin1', on_bad_lines='skip')

            all_dfs.append(df_temp)
            
        except Exception as e:
            st.warning(f"Falha ao baixar um dos arquivos ({url}): {e}")

    if not all_dfs:
        return pd.DataFrame()
        
    # Combina Terra e Mar (se houver m√∫ltiplos arquivos)
    final_df = pd.concat(all_dfs, ignore_index=True)
    
    # Processa tudo junto
    return process_dataframe(final_df)

def to_excel(df):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False, sheet_name='Dados ANP')
        workbook = writer.book
        worksheet = writer.sheets['Dados ANP']
        (max_row, max_col) = df.shape
        column_settings = [{'header': column} for column in df.columns]
        worksheet.add_table(0, 0, max_row, max_col - 1, {'columns': column_settings})
        worksheet.set_column(0, max_col - 1, 15)
    return output.getvalue()

# --- MAIN APP ---

def main():
    st.set_page_config(page_title=PAGE_TITLE, layout="wide")
    st.title(PAGE_TITLE)
    
    st.sidebar.header("Configura√ß√µes")
    
    # 1. Busca Anos
    with st.spinner("Varrendo site da ANP em busca de arquivos..."):
        available_years = get_available_years()
    
    if available_years:
        selected_year = st.sidebar.selectbox("Selecione o Ano", options=list(available_years.keys()))
        urls = available_years[selected_year]
        
        st.sidebar.success(f"Arquivos encontrados para {selected_year}: {len(urls)}")
        # Mostra quais arquivos ser√£o baixados (debug visual para o usu√°rio)
        with st.sidebar.expander("Ver links fonte"):
            for u in urls:
                st.write(u)
    else:
        st.warning("N√£o foi poss√≠vel encontrar anos automaticamente.")
        url_manual = st.sidebar.text_input("Cole a URL do CSV manualmente")
        urls = [url_manual] if url_manual else []
        selected_year = "Manual"

    # 2. Bot√£o de Download
    if st.sidebar.button("Baixar/Atualizar Dados"):
        if urls:
            st.session_state['data'] = load_data(urls)
            st.session_state['year'] = selected_year
        else:
            st.error("Nenhuma URL v√°lida para baixar.")

    # 3. Visualiza√ß√£o
    if 'data' in st.session_state and not st.session_state['data'].empty:
        df = st.session_state['data']
        
        st.markdown(f"### üìä Dados Consolidados: {st.session_state.get('year', 'N/A')}")
        
        # Filtros
        col1, col2 = st.columns(2)
        filtered_df = df.copy()
        
        if "Campo" in filtered_df.columns:
            campos = sorted(filtered_df["Campo"].dropna().astype(str).unique().tolist())
            selected_campos = col1.multiselect("Filtrar por Campo", options=campos)
            if selected_campos:
                filtered_df = filtered_df[filtered_df["Campo"].isin(selected_campos)]
        
        if "Po√ßo" in filtered_df.columns:
            # Filtra po√ßos baseado no campo selecionado (se houver)
            pocos_disponiveis = sorted(filtered_df["Po√ßo"].dropna().astype(str).unique().tolist())
            selected_pocos = col2.multiselect("Filtrar por Po√ßo", options=pocos_disponiveis)
            if selected_pocos:
                filtered_df = filtered_df[filtered_df["Po√ßo"].isin(selected_pocos)]
        
        st.info(f"Exibindo {len(filtered_df)} registros de {len(df)} totais.")
        st.dataframe(filtered_df, use_container_width=True)
        
        if not filtered_df.empty:
            st.download_button(
                label="üì• Baixar Excel (.xlsx)",
                data=to_excel(filtered_df),
                file_name=f"producao_anp_{st.session_state.get('year', 'dados')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
            
    elif 'data' in st.session_state:
        st.warning("Arquivo baixado, mas est√° vazio.")

if __name__ == "__main__":
    main()
```[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQEizbIMVWRbgc1Nj2a8k5WNiWA7-tZX2AwPBZVN3EQjPlwbiAAr3CYVkoJBgUdi1GIpi4HDujB9hC5xHMLrgdTuKtvTMzTv8r95rfkYSmQSne8pQ9TNyaCjKx-C7PEx5DvfSYmct2DShQebnDi2dfMfbSY0q9XiuPuMfNvHJ-pCeee7ZafnMBliG3oLjQ%3D%3D)]




